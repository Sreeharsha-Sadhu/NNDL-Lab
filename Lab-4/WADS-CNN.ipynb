{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:09:53.484387Z",
     "start_time": "2025-02-14T09:09:53.480588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Required dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Tuple, List\n",
    "import pandas as pd"
   ],
   "id": "24ef08a3a7d3b513",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:10:58.355636Z",
     "start_time": "2025-02-14T09:10:52.632515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class WADSDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for WADS data\"\"\"\n",
    "    def __init__(self, velodyne_dir: str, labels_dir: str, transform=None):\n",
    "        self.velodyne_dir = velodyne_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.file_list = sorted([f for f in os.listdir(velodyne_dir) if f.endswith('.bin')])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def read_bin_file(self, file_path: str) -> np.ndarray:\n",
    "        \"\"\"Read binary point cloud file\"\"\"\n",
    "        try:\n",
    "            return np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading bin file {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def read_label_file(self, file_path: str) -> np.ndarray:\n",
    "        \"\"\"Read label file\"\"\"\n",
    "        try:\n",
    "            return np.fromfile(file_path, dtype=np.uint32)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading label file {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def convert_to_2d_representation(self, point_cloud: np.ndarray,\n",
    "                                     height: int = 64, width: int = 512) -> np.ndarray:\n",
    "        \"\"\"Convert point cloud to 2D representation\"\"\"\n",
    "        # Extract x, y, z coordinates\n",
    "        x, y, z = point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2]\n",
    "\n",
    "        # Calculate polar coordinates\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        theta = np.arctan2(y, x)\n",
    "        phi = np.arctan2(z, r)\n",
    "\n",
    "        # Normalize angles to [0, 1]\n",
    "        theta_normalized = (theta + np.pi) / (2 * np.pi)\n",
    "        phi_normalized = (phi + np.pi/2) / np.pi\n",
    "\n",
    "        # Convert to pixel coordinates\n",
    "        x_pixel = (theta_normalized * width).astype(int)\n",
    "        y_pixel = (phi_normalized * height).astype(int)\n",
    "\n",
    "        # Clip values to prevent out of bounds\n",
    "        x_pixel = np.clip(x_pixel, 0, width-1)\n",
    "        y_pixel = np.clip(y_pixel, 0, height-1)\n",
    "\n",
    "        # Create image representation\n",
    "        image = np.zeros((height, width, 3))\n",
    "        image[y_pixel, x_pixel, 0] = r / np.max(r)  # normalized radius\n",
    "        image[y_pixel, x_pixel, 1] = z / np.max(np.abs(z))  # normalized height\n",
    "        image[y_pixel, x_pixel, 2] = point_cloud[:, 3]  # intensity\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get a single item from the dataset\"\"\"\n",
    "        try:\n",
    "            velodyne_path = os.path.join(self.velodyne_dir, self.file_list[idx])\n",
    "            label_path = os.path.join(self.labels_dir,\n",
    "                                      self.file_list[idx].replace('.bin', '.label'))\n",
    "\n",
    "            # Read data\n",
    "            point_cloud = self.read_bin_file(velodyne_path)\n",
    "            labels = self.read_label_file(label_path)\n",
    "\n",
    "            # Convert to 2D representation\n",
    "            image = self.convert_to_2d_representation(point_cloud)\n",
    "\n",
    "            # Apply transforms if any\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            # Convert to torch tensors\n",
    "            image = torch.FloatTensor(image).permute(2, 0, 1)  # Convert to CxHxW format\n",
    "            labels = torch.LongTensor(labels)\n",
    "\n",
    "            return image, labels\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing item {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"CNN model for WADS classification\"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Class to handle model training and evaluation\"\"\"\n",
    "    def __init__(self, model: nn.Module, device: torch.device,\n",
    "                 criterion: nn.Module, optimizer: torch.optim.Optimizer):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(val_loader, desc=\"Validating\"):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader,\n",
    "              epochs: int) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"Train the model for specified number of epochs\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            logger.info(f\"Training Loss: {train_loss:.4f}\")\n",
    "            logger.info(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        return self.train_losses, self.val_losses\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot training and validation losses\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_filters(self):\n",
    "        \"\"\"Visualize filters from first convolutional layer\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get the first conv layer weights\n",
    "            first_layer = self.model.features[0]\n",
    "            weights = first_layer.weight.cpu().numpy()\n",
    "\n",
    "            # Plot filters\n",
    "            fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "            for i, ax in enumerate(axes.flat):\n",
    "                if i < weights.shape[0]:\n",
    "                    ax.imshow(weights[i, 0], cmap='viridis')\n",
    "                ax.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Dataset parameters\n",
    "    VELODYNE_DIR = \"./WADS-11/velodyne\"\n",
    "    LABELS_DIR = \"./WADS-11/labels\"\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 4\n",
    "    NUM_CLASSES = 20  # Adjust based on your dataset\n",
    "\n",
    "    # Training parameters\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 20\n",
    "\n",
    "    # Create dataset and dataloaders\n",
    "    dataset = WADSDataset(VELODYNE_DIR, LABELS_DIR)\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Create model and training components\n",
    "    model = CNNModel(num_classes=NUM_CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Create trainer and train model\n",
    "    trainer = ModelTrainer(model, device, criterion, optimizer)\n",
    "    train_losses, val_losses = trainer.train(train_loader, val_loader, EPOCHS)\n",
    "\n",
    "    # Plot results\n",
    "    trainer.plot_losses()\n",
    "    trainer.visualize_filters()\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'wads_model.pth')\n",
    "\n",
    "    logger.info(\"Training completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        raise"
   ],
   "id": "8cfc63854e3ba23b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 14:40:52,646 - INFO - Using device: cuda\n",
      "2025-02-14 14:40:52,886 - INFO - Epoch 1/20\n",
      "Training:   0%|          | 0/3 [00:05<?, ?it/s]\n",
      "2025-02-14 14:40:57,919 - ERROR - An error occurred: DataLoader worker (pid(s) 16412, 12664, 6856, 16212) exited unexpectedly\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 16412, 12664, 6856, 16212) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "File \u001B[1;32mF:\\DevCache\\conda\\envs\\main\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1242\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1243\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_queue\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m   1244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[1;32mF:\\DevCache\\conda\\envs\\main\\Lib\\multiprocessing\\queues.py:114\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout):\n\u001B[1;32m--> 114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 270\u001B[0m\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    269\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 270\u001B[0m         main()\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    272\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[34], line 257\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;66;03m# Create trainer and train model\u001B[39;00m\n\u001B[0;32m    256\u001B[0m trainer \u001B[38;5;241m=\u001B[39m ModelTrainer(model, device, criterion, optimizer)\n\u001B[1;32m--> 257\u001B[0m train_losses, val_losses \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mtrain(train_loader, val_loader, EPOCHS)\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# Plot results\u001B[39;00m\n\u001B[0;32m    260\u001B[0m trainer\u001B[38;5;241m.\u001B[39mplot_losses()\n",
      "Cell \u001B[1;32mIn[34], line 180\u001B[0m, in \u001B[0;36mModelTrainer.train\u001B[1;34m(self, train_loader, val_loader, epochs)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m    178\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 180\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_epoch(train_loader)\n\u001B[0;32m    181\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate(val_loader)\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "Cell \u001B[1;32mIn[34], line 146\u001B[0m, in \u001B[0;36mModelTrainer.train_epoch\u001B[1;34m(self, train_loader)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m    144\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 146\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (data, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tqdm(train_loader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining\u001B[39m\u001B[38;5;124m\"\u001B[39m)):\n\u001B[0;32m    147\u001B[0m     data, target \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice), target\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32mF:\\DevCache\\conda\\envs\\main\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32mF:\\DevCache\\conda\\envs\\main\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mF:\\DevCache\\conda\\envs\\main\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1448\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_data()\n\u001B[0;32m   1449\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1451\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32mF:\\DevCache\\conda\\envs\\main\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1408\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1409\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1410\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1411\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1412\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_get_data()\n\u001B[0;32m   1413\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1414\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32mF:\\DevCache\\conda\\envs\\main\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1255\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[1;32m-> 1256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1257\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpids_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1258\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   1259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[0;32m   1260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 16412, 12664, 6856, 16212) exited unexpectedly"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
